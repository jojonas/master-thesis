% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../document.tex

\chapter{Conclusion}
The \acf{MUSiC} is a complex analysis with the ultimate goal to discover new physics in \ac{LHC} data. It consists of a multitude of algorithms and parameters which have to be implemented and optimized by the analyst. This process is usually guided by physical intuition and manual inspection of analysis results. However, it is important to regularly perform a systematic reevaluation of the state of the analysis and  study its discovery potential.
In this thesis, the framework for such a study has been refined and subsequently applied to existing features as well as newly introduced extensions of the analysis.

Among the evaluated features was the inclusion of \Pqb-tagged jets as dedicated physics objects, which has not been part of the analysis since the increase in collision energy to $\sqrt{s} = \SI{13}{\TeV}$ in 2015. Although the increase in sensitivity could not be explicitly shown on the provided benchmark models, the feature is predicted to increase sensitivity towards new physics and therefore should be further investigated in the future.

On the technical side of the analysis, the complexity of the tool chain has grown. This has motivated employing further automation, through which it was possible to cope with the additional workload posed by the exploration of the multidimensional parameter space.
Additionally, a \acl{LUT} has been implemented and evaluated in order to optimize the performance of the automated search for deviations.

Taking up discussions published in a thesis eight years ago\cite{Schmitz:ModelUnspecificSearch}, the potential of a log-normal prior within the local test statistic has been evaluated. For this purpose, a study of the coverage behavior of both the Gaussian- and log-normal prior has been performed, indicating that the log-normal option features superior coverage properties. However, several difficulties regarding pseudo-experiments with a log-normal prior arise. Possible mitigations have been discussed, and it was decided to maintain using the Gaussian prior with additional constraints on the search space.

Another new feature within the analysis is the computation of a global $p$-value. It allows to quantify deviations within the distribution of \ptilde-values, instead of judging deviations by eye. The feature enables future analysts to observe whether a certain change of a parameter value or the implementation of a new feature increases the sensitivity towards certain benchmark models.

Finally, the sensitivity of the analysis towards four models of new physics has been assessed. These benchmark models have been chosen to cover a wide range of possible signatures of new physics. By combining simulated events of these models with \acl{SM} processes, generating pseudo-experiments and analyzing the resulting distributions with the automated search, the impact of each model on the statistical inference has been evaluated. 
In two instances, the \PWprime and the Seesaw Type-III model,  the sensitivity did not (or barely) suffice to discover the presence of the simulated events. In these cases, a comparison to the dedicated analyses was drawn, yielding several suggestions for features that may increase the sensitivity.
The discovery potential for the two other models, semiclassical and quantum black holes, could successfully be shown. The highest discoverable object masses are in agreement with mass limits from corresponding dedicated analyses.

In the future, I would like to encourage analysts who pursue a model independent analysis approach to regularly reevaluate the sensitivity of their analysis to a wide range of new physics simulations. Furthermore, I would suggest to refine the idea of a global $p$-value and a measure of sensitivity, as informed decisions will help the analysis to remain lean and efficient for discovering new physics.

%automation
%bJets
%LUT
%coverage
%evaluation of lognormal
%phat